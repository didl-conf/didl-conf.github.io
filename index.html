<!doctype html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Second Workshop on Distributed Infrastructures for Deep Learning (DIDL) 2018</title>
  <meta name="description" content="Distributed Deep Learning">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
    crossorigin="anonymous">

  <!-- Optional theme -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp"
    crossorigin="anonymous">

  <!-- Latest compiled and minified JavaScript -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
    crossorigin="anonymous"></script>

   <link href="blog.css" rel="stylesheet">

   <!--
  <style>
	table, th, td { border: 1px solid black; }
  </style>
  -->

</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-16788810-5', 'auto');
  ga('send', 'pageview');

</script>

<body>

      <div class="container" style="max-width:940px  !important;">
      <div class="blog-masthead">
  <p>
    <a href="http://2018.middleware-conference.org/"><img src="header.jpeg" width="940" align='center' ></img></a>
  </p>
      </div>
    </div>


    <div class="container" style="max-width:940px  !important;">

      <div class="blog-header">
        <h1 class="blog-title">Second Workshop on Distributed Infrastructures for Deep Learning (DIDL) 2018</h1>
        <p class="lead blog-description"><a href="http://2018.middleware-conference.org/index.php/workshops/">Middleware 2018 Workshop</a></p>
        <p class="blog-description"> </p>
      </div>

      <div class="row">
        <div class="col-sm-12 blog-main">
     <div class="blog-post">

<p align="justify">
Deep learning is a rapidly growing field of machine learning, and has proven successful in many domains, including computer vision, language translation, and speech recognition. The training of deep neural networks is resource intensive, requiring compute accelerators such as GPUs, as well as large amounts of storage and memory, and network bandwidth. Additionally, getting the training data ready requires a lot of tooling for data cleansing, data merging, ambiguity resolution, etc. Sophisticated middleware abstractions are needed to schedule resources, manage the distributed training job as well as visualize how well the training is progressing. Likewise, serving the large neural network models with low latency constraints can require middleware to manage model caching, selection, and refinement.
<p align="justify">
All the major cloud providers, including Amazon, Google, IBM, and  Microsoft have started to offer cloud services in the last year or so with services to train and/or serve deep neural network models. In addition, there is a lot of activity in open source middleware for deep learning, including Tensorflow, Theano, Caffe2, PyTorch, and MXNet. There are also efforts to extend existing platforms such as Spark for deep learning workloads.
<p align="justify">
This workshop focuses on the tools, frameworks, and algorithms to support executing deep learning algorithms in a distributed environment. As new hardware and accelerators become available, the middleware and systems need to be able exploit their capabilities and ensure they are utilized efficiently.
<p align="justify">


<h2>Agenda</h2>

<p>The following papers have been accepted:

<p><strong>Introduction and tutorial on deep learning (9:00 - 9:30)</strong></br>
Bishwaranjan Bhattacharjee (IBM Research)

<p><strong>Keynote #1: Applications of deep learning at SAP (9:30 - 10:30)</strong></br>
Zbigniew Jerzak (SAP)

<p> Zbigniew Jerzak is the Head of the Deep Learning Center of Excellence and Machine Learning Research at SAP. The mission of the Deep Learning Center of Excellence and Machine Learning Research is to research and develop machine learning technology behind existing and new SAP products. Our technology is serving hundreds of SAP customers and is touching millions of transactions every month. In his previous roles at SAP Zbigniew has been responsible for a number of research and development projects covering, among others: columnar store database engineering, elastic data stream processing, and data visualization. Zbigniew holds a PhD degree in Distributed Systems from the TU Dresden, Germany.

<p><strong>Break (10:30 - 10:00)</strong></br>

<p><strong>Paper presentations #1 (11:00 - 12:00)</strong></br>

<p><i>A performance evaluation of federated learning algorithms</i></br>
Adrian Nilsson, Simon Smith, Gregor Ulm, Emil Gustavsson, Mats Jirstrand (Fraunhofer-Chalmers Centre & Fraunhofer Center for Machine Learning)

<p><i>Distributed C++-Python embedding for fast predictions and fast prototyping</i></br>
Georgios Varisteas (University of Luxembourg), Tigran Avanesov (OlaMobile), Radu State (University of Luxembourg)

<p><strong>Lunch (12:00 - 1:30)</strong></br>

<p><strong>Keynote #2: Robust scheduling and elastic scaling of deep learning workloads (1:30 - 2:30)</strong></br>
K. R. Jayaram (IBM Research)

<p><strong>Paper presentations #2 (2:30 - 3:00)</strong></br>

<p><i>Parallelized training of deep NN – comparison of current concepts and frameworks</i></br>
Sebastian Jäger (inovex GmbH, Karlsruhe, Germany), Stefan Igel (inovex GmbH, Karlsruhe, Germany), Christian Zirpins (Karlsruhe University of Applied Sciences), Hans-Peter Zorn (inovex GmbH, Karlsruhe, Germany)

<p><strong>Break (3:00 - 3:30)</strong></br>

<p><strong>Paper presentations #3 (3:30 - 4:15)</strong></br>

<p><i>Object Storage for Deep Learning Frameworks</i></br>
Or Ozeri, Effi Ofer, Ronen Kat (IBM Research)

<p><i>Gossiping GANs</i></br>
Hardy Corentin (INRIA/Technicolor), Le Merrer Erwan (Technicolor), Sericola Bruno (INRIA)

<p><strong>Closing remarks (4:15 - 4:30)</strong></br>


<h2>Workshop call for papers</h2>
<a href="cfp.html">Call For Papers (CFP)</a>

  <h2>
    Workshop Co-chairs
  </h2>

  <p>
    Bishwaranjan Bhattacharjee, IBM Research<br/>
    Vatche Ishakian, Bentley University<br/>
    Vinod Muthusamy, IBM Research<br/>
  </p>

  <h2>
    Program Committee
  </h2>

  <p>
    Parag Chandakkar, Walmart Labs <br/>
    Ian Foster, Argonne National Laboratory and the University of Chicago <br/>
    Benoit Huet, Eurecom <br/>
    Gauri Joshi, Carnegie Mellon University<br/>
    Ruben Mayer, Technical University of Munich<br/>
    Pietro Michiardi, Eurecom<br/>
    Peter Pietzuch, Imperial College<br/>
    Evgenia Smirni, College of William and Mary<br/>
    Yandong Wang, Citadel Securities<br/>
    Chuan Wu, University of Hong Kong<br/>
  </p>


         <p>
    &nbsp;
  </p>
         <p>
    &nbsp;
  </p>



          </div><!-- /.blog-post -->
      </div><!-- /.row -->

    </div><!-- /.container -->

</body>

</html>
