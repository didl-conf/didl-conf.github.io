<!doctype html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DIDL 2020 - Fourth Workshop on Distributed Infrastructures for Deep Learning</title>
  <meta name="description" content="Distributed Deep Learning">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
    crossorigin="anonymous">

  <!-- Optional theme -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp"
    crossorigin="anonymous">

  <!-- Latest compiled and minified JavaScript -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
    crossorigin="anonymous"></script>

   <link href="blog.css" rel="stylesheet">

   <!--
  <style>
	table, th, td { border: 1px solid black; }
  </style>
  -->

</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-16788810-5', 'auto');
  ga('send', 'pageview');

</script>

<body>

      <div class="container" style="max-width:900px  !important;">
        <div style="max-width:940px  !important;">
          <p align='center'>
    <a href="http://2020.middleware-conference.org/"><img src="header.jpg" align='center' style="width:700px; height:auto"></img></a>
  </p>
      </div>
    </div>


    <div class="container" style="max-width:900px  !important;">

      <div class="blog-header">
        <h1 class="blog-title">Fourth Workshop on Distributed Infrastructures for Deep Learning (DIDL) 2020</h1>
        <p class="lead blog-description"><!--<a href="http://2020.middleware-conference.org/index.php/workshops/">-->Middleware 2020 Workshops<!--</a>--></p>
        <p class="blog-description"> </p>
      </div>

      <div class="row">
        <div class="col-sm-12 blog-main">
     <div class="blog-post">

<p align="justify">
The DIDL workshop is co-located with <a href="http://2020.middleware-conference.org">ACM/IFIP Middleware 2020</a>, which takes place from December 7-11 in Delft, The Netherlands.

<p align="justify">
Deep learning is a rapidly growing field of machine learning, and has proven successful in many domains, including computer vision, language translation, and speech recognition. The training of deep neural networks is resource intensive, requiring compute accelerators such as GPUs, as well as large amounts of storage and memory, and network bandwidth. Additionally, getting the training data ready requires a lot of tooling for data cleansing, data merging, ambiguity resolution, etc. Sophisticated middleware abstractions are needed to schedule resources, manage the distributed training job as well as visualize how well the training is progressing. Likewise, serving the large neural network models with low latency constraints can require middleware to manage model caching, selection, and refinement.

<p align="justify">
All the major cloud providers, including Amazon, Google, IBM, and  Microsoft have started to offer cloud services in the last year or so with services to train and/or serve deep neural network models. In addition, there is a lot of activity in open source middleware for deep learning, including Tensorflow, Theano, Caffe2, PyTorch, and MXNet. There are also efforts to extend existing platforms such as Spark for deep learning workloads.

<p align="justify">
This workshop focuses on the tools, frameworks, and algorithms to support executing deep learning algorithms in a distributed environment. As new hardware and accelerators become available, the middleware and systems need to be able exploit their capabilities and ensure they are utilized efficiently.

<p align="justify">
  The workshop is scheduled to be in the <b>morning on Dec 7, 2020</b>.

<h2>Workshop Agenda</h2>

<p><strong>Introduction (9:00 - 9:10 ET / 15:00 - 15:10 CET)</strong></br>

<p><strong>Keynote: Making Training in Distributed Machine Learning Adaptive (9:10 - 9:55 ET / 15:10 - 15:55 CET)</strong></br>
  Prof. Peter Pietzuch, Imperial College London
  <p> 
  <p><strong>Abstract:</strong> When using distributed machine learning (ML) systems to train models on
    a cluster of worker machines, users must configure a large number of
    parameters: hyper-parameters (e.g. the batch size and the learning rate)
    affect model convergence; system parameters (e.g. the number of workers
    and their communication topology) impact training performance. In
    current systems, adapting such parameters during training is ill-supported.

    In this talk, I will describe our recent work on KungFu, a distributed
    ML library for TensorFlow and PyTorch that is designed to enable
    adaptive training. KungFu allows users to express high-level Adaptation
    Policies (APs) that describe how to change hyper- and system parameters
    during training. APs take real-time monitored metrics (e.g.
    signal-to-noise ratios and noise scale) as input and trigger control
    actions (e.g. cluster rescaling or synchronisation strategy updates).
    For execution, APs are translated into monitoring and control operators,
    which are embedded in the dataflow graph. APs exploit an efficient
    asynchronous collective communication layer, which ensures concurrency
    and consistency of monitoring and adaptation operations. (This work has appeared in USENIX OSDI 2020.)</p>

  <p><strong>Bio:</strong>
    Peter Pietzuch is a Professor of Distributed Systems at Imperial College
    London, where he leads the Large-scale Data & Systems (LSDS) group
    (https://lsds.doc.ic.ac.uk). His research work focuses on the design and
    engineering of scalable, reliable and secure large-scale software
    systems, with a particular interest in performance, data management and
    security issues when supporting machine learning applications. He has
    published papers in premier scientific venues, including OSDI/SOSP,
    SIGMOD, VLDB, ASPLOS, USENIX ATC, EuroSys, SoCC, ICDCS, DEBS, and
    Middleware. Currently he is a Visiting Researcher with Microsoft
    Research and serves as the Director of Research in the Department, the
    Chair of the ACM SIGOPS European Chapter, and an Associate Editor for
    IEEE TKDE and TCC. Before joining Imperial College London, he was a
    post-doctoral Fellow at Harvard University. He holds PhD and MA degrees
    from the University of Cambridge.
  </p>

<p><strong>Break (9:55 - 10:05 ET / 15:55 - 16:05 CET)</strong></br>

<p><strong>Paper presentations (10:05 - 11:05 ET / 16:05 - 17:05 CET)</strong></br>

  <p><i><a href=""></a>Graph Representation Matters in Device Placement</a></i></br>
    Milko Mitropolitsky, Zainab Abbas,Amir H. Payberah (KTH Royal Institute of Technology)</br>

  <p><i><a href=""></a>Tools and Techniques for Privacy-aware, Edge-centric Distributed Deep Learning</i></br>
    Ziran Min*, Robert E. Canady*, Akram Hakiri^, Uttam Ghosh*, Aniruddha Gokhale*<br>
    * Vanderbilt University  ^ University of Carthage</br>

<p><strong>Break (11:05 - 11:15 ET / 17:05 - 17:15 CET)</strong></br>

<p><strong>Keynote: Architecture Transferability in Large Scale Neural Architecture Search (11:15 - 12:00 ET / 17:15 - 18:00 CET)</strong></br>
    Rameshwar Panda, IBM Research
    <p> 
    <p><strong>Abstract:</strong> Neural Architecture Search (NAS) is an open and challenging problem in machine learning. While NAS offers great promise, the prohibitive computational demand of most of the existing NAS methods makes it difficult to directly search the architectures on large-scale tasks. The typical way of conducting large scale NAS is to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. In this talk, I will briefly review recent progress and challenges in the architecture transferability of different NAS methods, discuss transfer value of different proxy datasets, and few directions that machine learning researchers should focus in designing future NAS algorithms that are not only efficient but also more effective at large scale.</p>
  
    <p><strong>Bio:</strong> Rameswar Panda is currently a Research Staff Member at MIT-IBM Watson AI Lab, Cambridge, USA. Prior to joining MIT-IBM lab, he obtained his Ph.D in Electrical and Computer Engineering from University of California, Riverside in 2018. During Ph.D., Rameswar worked at NEC Labs America, Adobe Research and Siemens Corporate Research. His primary research interests span the areas of computer vision, machine learning and multimedia. In particular, his current focus is on image and video understanding including efficient dynamic neural networks, large-scale neural architecture search and learning with limited supervision. His work has been published in top-tier conferences such as CVPR, ICCV, ECCV, NeurIPS as well as high impact journals such as TIP and TMM. He actively participates as a program committee member for many top AI conferences and was leading co-chair of the workshop on Multi-modal Video Analysis at ECCV 2020 and Workshop on Neural Architecture Search at CVPR 2020. More details can be found in https://rpand002.github.io/.
<p></p>

<p><strong>Workshop conclusion (12:00 - 12:10 ET / 18:00 - 18:10 CET)</strong></br>

<h2>
    Workshop call for papers
  </h2>
  <p><a href="cfp.html">Call For Papers (CFP)</a></p>

  <h2>
    Workshop Co-chairs
  </h2>

  <p>
    Bishwaranjan Bhattacharjee, IBM Research<br/>
    Vatche Ishakian, Bentley University<br/>
    Vinod Muthusamy, IBM Research<br/>
  </p>

  <h2>
    Program Committee
  </h2>

  <p>
    Parag Chandakkar, Walmart Labs <br/>
    Ian Foster, Argonne National Laboratory and the University of Chicago <br/>
    Matthew Hill, Dataminr <br/>
    Mayoore Jaiswal, Nvidia <br/>
    Gauri Joshi, Carnegie Mellon University<br/>
    Jayaram K. R., IBM Research<br/>
    Ruben Mayer, Technical University of Munich<br/>
    Pietro Michiardi, Eurecom<br/>
    Phuong Nguyen, eBay <br/>
    Peter Pietzuch, Imperial College<br/>
    Chuan Wu, University of Hong Kong<br/>
  </p>


         <p>
    &nbsp;
  </p>
         <p>
    &nbsp;
  </p>



          </div><!-- /.blog-post -->
      </div><!-- /.row -->

    </div><!-- /.container -->

</body>

</html>
